version: '3.8'

services:
  # Apache Kafka Cluster
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log

  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka1
    container_name: kafka1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka1:29092
      KAFKA_CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      KAFKA_CONFLUENT_METRICS_ENABLE: 'true'
      KAFKA_CONFLUENT_SUPPORT_CUSTOMER_ID: anonymous
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    volumes:
      - kafka1-data:/var/lib/kafka/data

  kafka2:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka2
    container_name: kafka2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
      - "29093:29093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:29093,PLAINTEXT_HOST://localhost:9093
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka2:29093
      KAFKA_CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      KAFKA_CONFLUENT_METRICS_ENABLE: 'true'
      KAFKA_CONFLUENT_SUPPORT_CUSTOMER_ID: anonymous
    volumes:
      - kafka2-data:/var/lib/kafka/data

  # Kafka Connect for data integration
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.4.0
    hostname: connect
    container_name: connect
    depends_on:
      - kafka1
      - kafka2
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka1:29092,kafka2:29093'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    volumes:
      - ./kafka/connectors:/etc/kafka-connect/jars

  # Apache Spark Master
  spark-master:
    image: bitnami/spark:3.4.1
    hostname: spark-master
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8080"  # Spark Master Web UI (changed to avoid conflict)
      - "7077:7077"  # Spark Master Port
    volumes:
      - ./spark:/opt/spark-apps
      - spark-logs:/opt/spark/logs

  # Spark Worker 1
  spark-worker-1:
    image: bitnami/spark:3.4.1
    hostname: spark-worker-1
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark:/opt/spark-apps
      - spark-logs:/opt/spark/logs

  # Spark Worker 2
  spark-worker-2:
    image: bitnami/spark:3.4.1
    hostname: spark-worker-2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark:/opt/spark-apps
      - spark-logs:/opt/spark/logs

  # PostgreSQL for metadata and configuration
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_DB: loganalytics
      POSTGRES_USER: loguser
      POSTGRES_PASSWORD: logpass
    ports:
      - "5433:5432"  # Changed to avoid conflict with MLflow postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U loguser -d loganalytics"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis for caching and real-time data
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6380:6379"  # Changed to avoid conflict with MLflow redis
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes

  # FastAPI Gateway Service
  kafka-gateway:
    build: ./backend/kafka-gateway
    container_name: kafka-gateway
    ports:
      - "8000:8000"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:29092,kafka2:29093
      - DATABASE_URL=postgresql://loguser:logpass@postgres:5432/loganalytics
      - REDIS_URL=redis://redis:6379
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - kafka1
      - kafka2
      - postgres
      - redis
      - spark-master
    volumes:
      - ./backend/kafka-gateway:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # Spark Job Manager Service
  spark-manager:
    build: ./backend/spark-manager
    container_name: spark-manager
    ports:
      - "8001:8000"
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:29092,kafka2:29093
      - DATABASE_URL=postgresql://loguser:logpass@postgres:5432/loganalytics
      - REDIS_URL=redis://redis:6379
    depends_on:
      - spark-master
      - kafka1
      - postgres
    volumes:
      - ./backend/spark-manager:/app
      - ./spark:/opt/spark-apps
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # React Dashboard
  react-dashboard:
    build: ./react-dashboard
    container_name: react-dashboard
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_SPARK_API_URL=http://localhost:8001
      - REACT_APP_KAFKA_WS_URL=ws://localhost:8000/ws
    depends_on:
      - kafka-gateway
      - spark-manager
    volumes:
      - ./react-dashboard:/app
      - /app/node_modules
    command: npm start

  # Kafka UI for monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka1
      - kafka2
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:29092,kafka2:29093
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:v2.44.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.0.0
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus

  # Log Generator (for testing)
  log-generator:
    build: ./scripts/log-generator
    container_name: log-generator
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:29092,kafka2:29093
      - LOG_RATE=100  # logs per second
    depends_on:
      - kafka1
      - kafka2
    volumes:
      - ./scripts/log-generator:/app

  # MLflow Services
  mlflow-postgres:
    image: postgres:15
    container_name: mlflow-postgres
    environment:
      POSTGRES_DB: log_analytics
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  mlflow-redis:
    image: redis:7
    container_name: mlflow-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  mlflow:
    image: python:3.12
    container_name: mlflow
    command: >
      sh -c "pip install mlflow==2.8.1 && 
             mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root ./artifacts"
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/app/artifacts
    depends_on:
      - mlflow-postgres
      - mlflow-redis

volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka1-data:
  kafka2-data:
  postgres-data:
  redis-data:
  spark-logs:
  prometheus-data:
  grafana-data:
  postgres_data:
  redis_data:
  mlflow_data: